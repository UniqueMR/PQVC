- All-reduce: 结合来自所有进程（例如所有GPU）的数据（通常通过求和或平均等归约操作），然后将最终的组合结果分发给所有进程。例如分布式训练中的梯度同步
- All-Gather: 每个进程拥有数据的一部分，all-gather将所有进程的数据片段收集起来，并将完整的、拼接后的数据分发给每个进程
- All-to-All: 每个进程都向其他所有进程发送其数据块的不同部分，并且也从其他所有进程接收数据块的不同部分。矩阵转置，每个进程持有矩阵的一行，操作后每个进程持有矩阵的一列
- Reduce-Scatter: All-reduce + 分割最终的归约结果成块，然后分发（Scatter）给不同的进程